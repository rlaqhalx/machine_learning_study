{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the final version of the Ames Housing dataset I worked on through the feature engineering section of the course. I created a Linear Regression Model, trained it on the data with the optimal parameters using a grid search, and then evaluated the model's capabilities on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../DATA/AMES_Final_DF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lot Frontage</th>\n",
       "      <th>Lot Area</th>\n",
       "      <th>Overall Qual</th>\n",
       "      <th>Overall Cond</th>\n",
       "      <th>Year Built</th>\n",
       "      <th>Year Remod/Add</th>\n",
       "      <th>Mas Vnr Area</th>\n",
       "      <th>BsmtFin SF 1</th>\n",
       "      <th>BsmtFin SF 2</th>\n",
       "      <th>Bsmt Unf SF</th>\n",
       "      <th>...</th>\n",
       "      <th>Sale Type_ConLw</th>\n",
       "      <th>Sale Type_New</th>\n",
       "      <th>Sale Type_Oth</th>\n",
       "      <th>Sale Type_VWD</th>\n",
       "      <th>Sale Type_WD</th>\n",
       "      <th>Sale Condition_AdjLand</th>\n",
       "      <th>Sale Condition_Alloca</th>\n",
       "      <th>Sale Condition_Family</th>\n",
       "      <th>Sale Condition_Normal</th>\n",
       "      <th>Sale Condition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141.0</td>\n",
       "      <td>31770</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1960</td>\n",
       "      <td>1960</td>\n",
       "      <td>112.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1958</td>\n",
       "      <td>1958</td>\n",
       "      <td>108.0</td>\n",
       "      <td>923.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>406.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93.0</td>\n",
       "      <td>11160</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1968</td>\n",
       "      <td>1968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>791.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 274 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Lot Frontage  Lot Area  Overall Qual  Overall Cond  Year Built  \\\n",
       "0         141.0     31770             6             5        1960   \n",
       "1          80.0     11622             5             6        1961   \n",
       "2          81.0     14267             6             6        1958   \n",
       "3          93.0     11160             7             5        1968   \n",
       "4          74.0     13830             5             5        1997   \n",
       "\n",
       "   Year Remod/Add  Mas Vnr Area  BsmtFin SF 1  BsmtFin SF 2  Bsmt Unf SF  ...  \\\n",
       "0            1960         112.0         639.0           0.0        441.0  ...   \n",
       "1            1961           0.0         468.0         144.0        270.0  ...   \n",
       "2            1958         108.0         923.0           0.0        406.0  ...   \n",
       "3            1968           0.0        1065.0           0.0       1045.0  ...   \n",
       "4            1998           0.0         791.0           0.0        137.0  ...   \n",
       "\n",
       "   Sale Type_ConLw  Sale Type_New  Sale Type_Oth  Sale Type_VWD  \\\n",
       "0                0              0              0              0   \n",
       "1                0              0              0              0   \n",
       "2                0              0              0              0   \n",
       "3                0              0              0              0   \n",
       "4                0              0              0              0   \n",
       "\n",
       "   Sale Type_WD   Sale Condition_AdjLand  Sale Condition_Alloca  \\\n",
       "0              1                       0                      0   \n",
       "1              1                       0                      0   \n",
       "2              1                       0                      0   \n",
       "3              1                       0                      0   \n",
       "4              1                       0                      0   \n",
       "\n",
       "   Sale Condition_Family  Sale Condition_Normal  Sale Condition_Partial  \n",
       "0                      0                      1                       0  \n",
       "1                      0                      1                       0  \n",
       "2                      0                      1                       0  \n",
       "3                      0                      1                       0  \n",
       "4                      0                      1                       0  \n",
       "\n",
       "[5 rows x 274 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2925 entries, 0 to 2924\n",
      "Columns: 274 entries, Lot Frontage to Sale Condition_Partial\n",
      "dtypes: float64(11), int64(263)\n",
      "memory usage: 6.1 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separated out the data into X features and y labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('SalePrice', axis = 1)\n",
    "y = df['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split up X and y into a training set and test set. Since I will later be using a Grid Search strategy, I set my test proportion to 10%. To get the same data split as the solutions notebook, I specifed random_state = 101**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataset features has a variety of scales and units. For optimal regression performance, I scaled the X features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit\n",
    "    Purpose: The fit method is used to compute the necessary parameters (like mean and standard deviation for StandardScaler) from the training data (X_train).\n",
    "\n",
    "    Action: In the context of StandardScaler, fit calculates the mean and standard deviation for each feature in X_train.\n",
    "\n",
    "    When to Use: Training data so that the model can learn the parameters (mean and standard deviation) from it.\n",
    "\n",
    "### transform\n",
    "    Purpose: The transform method uses these parameters to transform the data. This transformation applies the learned parameters to scale (or normalize) the data.\n",
    "\n",
    "    Action: In StandardScaler, it subtracts the mean and divides by the standard deviation for each feature, using the parameters learned during the fit.\n",
    "\n",
    "    When to Use: Both training and test data, but the key point is that it uses the parameters learned from the training data only.\n",
    "\n",
    "### fit_transform\n",
    "    Purpose: This is a convenience method that combines fit and transform into one step.\n",
    "\n",
    "    Action: For StandardScaler, it first calculates the mean and standard deviation on the training data (X_train), and then it immediately scales the X_train data using these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elastic Net model for L1 and L2 Regularization (adding penalty to loss function for generalization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression (LinearRegression()):\n",
    "\n",
    "    Purpose: Linear Regression is used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.\n",
    "    \n",
    "    Method: The method involves finding the line that best fits the data, which is done by minimizing the sum of the squares of the differences between the observed and predicted values (known as the least squares method).\n",
    "    \n",
    "    Usage: It's generally used when there is a linear relationship between the variables and when the data is not prone to overfitting.\n",
    "    \n",
    "### Ridge Regression (Ridge(alpha)):\n",
    "\n",
    "    Purpose: Ridge Regression, also known as L2 regularization, is an extension of linear regression that adds a regularization term to the loss function.\n",
    "    \n",
    "    Method: In Ridge Regression, the loss function is the least squares loss plus a term proportional to the square of the magnitude of the coefficients. The proportionality constant is the alpha parameter. This additional term penalizes large coefficients and helps to reduce model complexity and prevent overfitting.\n",
    "    \n",
    "    Parameter alpha: The alpha parameter controls the strength of the regularization. When alpha is zero, Ridge Regression is equivalent to Linear Regression. As alpha increases, the impact of the regularization term grows, leading to smaller coefficients (but not zero).\n",
    "    \n",
    "    Usage: It's used when the data is prone to multicollinearity (independent variables are highly correlated) or when you want to prevent overfitting in a model with many features.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Technique used in machine learning to prevent overfitting, which occurs when a model learns the training data too well, including its noise and outliers, leading to poor performance on unseen data. Regularization does this by adding a penalty to the loss function used to train the model. This penalty discourages overly complex models and helps in generalizing the model to new, unseen data. \n",
    "\n",
    "The two most common types of regularization are L1 and L2 regularization.\n",
    "\n",
    "### L1 Regularization (Lasso Regression):\n",
    "\n",
    "    Mechanism: L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients. This is mathematically represented as the sum of the absolute values of the coefficients.\n",
    "\n",
    "    Effect: The key characteristic of L1 regularization is that it can lead to sparse models, where some coefficients can become exactly zero. This means L1 regularization can be used for feature selection, identifying which features are important for the prediction.\n",
    "\n",
    "\n",
    "### L2 Regularization (Ridge Regression):\n",
    "\n",
    "    Mechanism: L2 regularization adds a penalty equal to the square of the magnitude of coefficients. This is mathematically represented as the sum of the squares of the coefficients.\n",
    "\n",
    "    Effect: Unlike L1, L2 regularization does not lead to sparse models, and no coefficients are ever zeroed out. It only shrinks the size of the coefficients. L2 is good for handling multicollinearity and model complexity by distributing the error among all terms, but it doesn’t help in reducing the number of features.\n",
    "\n",
    "### Comparison:\n",
    "\n",
    "Feature Selection: L1 regularization can zero out coefficients, effectively performing feature selection. L2 does not.\n",
    "\n",
    "Stability: L2 regularization is more stable and less sensitive to outliers than L1.\n",
    "\n",
    "Usage: L1 is used when we have a high number of features, and we need to determine which are important. L2 is used when we need to prevent multicollinearity or when we have overfitting but all features are important.\n",
    "\n",
    "### Combining L1 and L2 (Elastic Net):\n",
    "\n",
    "Combination of L1 and L2 Regularization: \n",
    "Elastic Net includes both the L1 and L2 penalty terms in its regularization formula. \n",
    "The L1 term makes it capable of reducing the coefficients of less important features to zero (like Lasso), facilitating feature selection.\n",
    "The L2 term shrinks the coefficients of correlated predictors (like Ridge), which helps in handling multicollinearity.\n",
    "\n",
    "Regularization Parameters: There are two key parameters in Elastic Net:\n",
    "\n",
    "    Alpha (α): Overall regularization strength. A larger value of α means more regularization.\n",
    "    \n",
    "    L1 Ratio: Determines the mix between L1 and L2 regularization. An L1 ratio of 1 is equivalent to Lasso, while an L1 ratio of 0 is equivalent to Ridge. Values in between 0 and 1 give both L1 and L2 effects.\n",
    "    \n",
    "Hyperparameter Tuning: Selecting the right values for α and the L1 ratio is crucial for the performance of the Elastic Net model. This is typically done via **cross-validation**.\n",
    "\n",
    "Standardization: Before applying Elastic Net, it's a good practice to **standardize** the features so that they are on a comparable scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_elastic_model = ElasticNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Elastic Net model has two main parameters, alpha and the L1 ratio. I created a dictionary parameter grid of values for the ElasticNet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha':[0.1,1,5,10,50,100],\n",
    "              'l1_ratio':[.1, .5, .7, .9, .95, .99, 1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I created a GridSearchCV object and ran a grid search for the best parameters for my model based on my scaled training data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV is a method that performs exhaustive search over a specified parameter values grid for an estimator. It's used to find the best combination of parameters from the provided param_grid.\n",
    "\n",
    "**Mean Squared Error (MSE):**\n",
    "\n",
    "This is a common metric used for regression tasks, which measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value.\n",
    "\n",
    "Why Negative?: In scikit-learn, many model selection tools are designed to choose the higher score as better. However, because MSE is a value that is better when lower (you want a small error), scikit-learn uses the negative of the MSE. This way, a model that results in a lower MSE will have a higher “negative MSE score” and be deemed better by selection tools like GridSearchCV.\n",
    "\n",
    "**Higher is Better:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbose number a personal preference\n",
    "grid_model = GridSearchCV(estimator=base_elastic_model,\n",
    "                          param_grid=param_grid,\n",
    "                          scoring='neg_mean_squared_error',\n",
    "                          cv=5,\n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.394e+11, tolerance: 1.355e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.654e+11, tolerance: 1.308e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.324e+11, tolerance: 1.415e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.986e+11, tolerance: 1.438e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.553e+11, tolerance: 1.346e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.235e+11, tolerance: 1.355e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.369e+11, tolerance: 1.308e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.575e+11, tolerance: 1.415e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.887e+11, tolerance: 1.438e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.928e+11, tolerance: 1.346e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.542e+11, tolerance: 1.355e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.635e+11, tolerance: 1.308e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.085e+11, tolerance: 1.415e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.071e+11, tolerance: 1.438e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.553e+11, tolerance: 1.346e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.498e+11, tolerance: 1.355e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.591e+11, tolerance: 1.308e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.066e+11, tolerance: 1.415e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.028e+11, tolerance: 1.438e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.687e+11, tolerance: 1.346e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.025e+11, tolerance: 1.355e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.127e+11, tolerance: 1.308e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.475e+11, tolerance: 1.415e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.526e+11, tolerance: 1.438e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.559e+11, tolerance: 1.346e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.707e+11, tolerance: 1.355e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.869e+11, tolerance: 1.308e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.965e+11, tolerance: 1.415e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.128e+11, tolerance: 1.438e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.849e+09, tolerance: 1.346e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.313e+10, tolerance: 1.355e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.414e+09, tolerance: 1.308e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.628e+10, tolerance: 1.415e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/bomi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.970e+09, tolerance: 1.438e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=ElasticNet(),\n",
       "             param_grid={'alpha': [0.1, 1, 5, 10, 50, 100],\n",
       "                         'l1_ratio': [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1]},\n",
       "             scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.fit(scaled_X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here I have displayed the best combination of parameters for my model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 100, 'l1_ratio': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is the evaluation for my model's performance on the unseen 10% scaled test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_model.predict(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14195.354900562172"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20558.508566893164"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180815.53743589742"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df['SalePrice'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
